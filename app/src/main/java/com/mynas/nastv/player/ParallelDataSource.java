package com.mynas.nastv.player;

import android.net.Uri;
import android.util.Log;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;
import androidx.media3.common.C;
import androidx.media3.datasource.BaseDataSource;
import androidx.media3.datasource.DataSpec;
import androidx.media3.datasource.HttpDataSource;
import androidx.media3.datasource.TransferListener;

import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.Map;
import java.util.concurrent.ArrayBlockingQueue;
import java.util.concurrent.BlockingQueue;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.atomic.AtomicBoolean;
import java.util.concurrent.atomic.AtomicLong;

import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.Response;

/**
 * ğŸš€ å¹¶è¡Œåˆ†å—ä¸‹è½½æ•°æ®æº
 * 
 * ä½¿ç”¨å¤šä¸ªè¿æ¥å¹¶è¡Œä¸‹è½½ä¸åŒçš„ Range å—ï¼ŒåŠ é€Ÿè§†é¢‘ç¼“å†²
 * ç±»ä¼¼äºä¸‹è½½åŠ é€Ÿå™¨çš„åŸç†
 */
public class ParallelDataSource extends BaseDataSource {
    
    private static final String TAG = "ParallelDataSource";
    
    // é…ç½®å‚æ•° - å¹³è¡¡å†…å­˜å’Œæµç•…åº¦
    private static final int CHUNK_SIZE = 512 * 1024;       // æ¯å—512KBï¼ˆå¹³è¡¡å†…å­˜å’Œæµç•…åº¦ï¼‰
    
    // åŠ¨æ€å‚æ•°ï¼ˆæ ¹æ®å†…å­˜è°ƒæ•´ï¼‰
    private int numConnections = 2;                         // å¹¶è¡Œè¿æ¥æ•°
    private int bufferChunks = 3;                           // ç¼“å†²åŒºå—æ•°
    private int prefetchChunks = 2;                         // é¢„å–å—æ•°
    
    private final OkHttpClient httpClient;
    private final Map<String, String> defaultHeaders;
    
    private Uri uri;
    private long contentLength = C.LENGTH_UNSET;
    private long currentPosition = 0;
    private long bytesRemaining = C.LENGTH_UNSET;
    
    // å¤šçº¿ç¨‹ä¸‹è½½
    private ExecutorService downloadExecutor;
    private BlockingQueue<ChunkData> chunkBuffer;  // åŠ¨æ€åˆ›å»º
    private final ConcurrentHashMap<Long, ChunkData> pendingChunks;  // å­˜å‚¨ä¹±åºåˆ°è¾¾çš„å—
    private final List<Future<?>> downloadTasks;
    private final AtomicBoolean isOpened;
    private final AtomicLong nextChunkToDownload;
    private final AtomicLong nextChunkToRead;
    
    // å½“å‰æ­£åœ¨è¯»å–çš„å—
    private ChunkData currentChunk;
    private int currentChunkOffset;
    
    public ParallelDataSource(OkHttpClient httpClient, Map<String, String> defaultHeaders) {
        super(/* isNetwork= */ true);
        this.httpClient = httpClient;
        this.defaultHeaders = defaultHeaders;
        this.pendingChunks = new ConcurrentHashMap<>();
        this.downloadTasks = new ArrayList<>();
        this.isOpened = new AtomicBoolean(false);
        this.nextChunkToDownload = new AtomicLong(0);
        this.nextChunkToRead = new AtomicLong(0);
    }
    
    @Override
    public long open(@NonNull DataSpec dataSpec) throws HttpDataSource.HttpDataSourceException {
        uri = dataSpec.uri;
        currentPosition = dataSpec.position;
        
        Log.d(TAG, "ğŸš€ Opening parallel data source: " + uri);
        Log.d(TAG, "ğŸš€ Start position: " + currentPosition);
        
        try {
            // ğŸ”‘ æ ¹æ®å¯ç”¨å†…å­˜åŠ¨æ€è°ƒæ•´å¹¶å‘å‚æ•°
            Runtime runtime = Runtime.getRuntime();
            long freeMemory = runtime.freeMemory();
            long maxMemory = runtime.maxMemory();
            long usedMemory = runtime.totalMemory() - freeMemory;
            long availableMemory = maxMemory - usedMemory;
            long availableMB = availableMemory / 1024 / 1024;
            
            Log.d(TAG, "ğŸš€ Memory: available=" + availableMB + "MB, max=" + (maxMemory / 1024 / 1024) + "MB");
            
            // æ ¹æ®å†…å­˜åŠ¨æ€è°ƒæ•´å‚æ•° - ä¿å®ˆç­–ç•¥é¿å…OOM
            if (availableMB >= 80) {
                // å†…å­˜å……è¶³ï¼š2è¿æ¥ï¼Œ4å—ç¼“å†²ï¼ˆæœ€å¤š2MBï¼‰
                numConnections = 2;
                bufferChunks = 4;
                prefetchChunks = 3;
            } else if (availableMB >= 40) {
                // å†…å­˜ä¸­ç­‰ï¼š2è¿æ¥ï¼Œ3å—ç¼“å†²ï¼ˆæœ€å¤š1.5MBï¼‰
                numConnections = 2;
                bufferChunks = 3;
                prefetchChunks = 2;
            } else {
                // å†…å­˜ç´§å¼ ï¼š1è¿æ¥ï¼Œ2å—ç¼“å†²ï¼ˆæœ€å¤š1MBï¼‰
                numConnections = 1;
                bufferChunks = 2;
                prefetchChunks = 1;
            }
            
            Log.d(TAG, "ğŸš€ Dynamic config: connections=" + numConnections + ", bufferChunks=" + bufferChunks);
            
            // åˆ›å»ºç¼“å†²åŒº
            chunkBuffer = new ArrayBlockingQueue<>(bufferChunks);
            
            // é¦–å…ˆè·å–æ–‡ä»¶æ€»å¤§å°
            contentLength = getContentLength();
            Log.d(TAG, "ğŸš€ Content length: " + contentLength);
            
            if (contentLength == C.LENGTH_UNSET) {
                // æ— æ³•è·å–å¤§å°ï¼Œå›é€€åˆ°å•è¿æ¥
                Log.w(TAG, "ğŸš€ Cannot get content length, falling back to single connection");
                return openSingleConnection(dataSpec);
            }
            
            // è®¡ç®—å‰©ä½™å­—èŠ‚
            if (dataSpec.length != C.LENGTH_UNSET) {
                bytesRemaining = dataSpec.length;
            } else {
                bytesRemaining = contentLength - currentPosition;
            }
            
            // åˆå§‹åŒ–å¤šçº¿ç¨‹ä¸‹è½½
            isOpened.set(true);
            nextChunkToDownload.set(currentPosition / CHUNK_SIZE);
            nextChunkToRead.set(currentPosition / CHUNK_SIZE);
            
            // åˆ›å»ºä¸‹è½½çº¿ç¨‹æ± 
            downloadExecutor = Executors.newFixedThreadPool(numConnections);
            
            // å¯åŠ¨é¢„å–
            startPrefetch();
            
            transferStarted(dataSpec);
            return bytesRemaining;
            
        } catch (Exception e) {
            Log.e(TAG, "ğŸš€ Open failed", e);
            throw new HttpDataSource.HttpDataSourceException(
                e instanceof IOException ? (IOException) e : new IOException(e), 
                dataSpec, 
                HttpDataSource.HttpDataSourceException.TYPE_OPEN);
        }
    }
    
    private long getContentLength() throws IOException {
        // ğŸ”‘ æœåŠ¡å™¨ä¸æ”¯æŒ HEAD è¯·æ±‚ï¼Œä½¿ç”¨ GET + Range: bytes=0-0 è·å–æ–‡ä»¶å¤§å°
        Request.Builder requestBuilder = new Request.Builder()
            .url(uri.toString())
            .get()
            .header("Range", "bytes=0-0");  // åªè¯·æ±‚ç¬¬ä¸€ä¸ªå­—èŠ‚
        
        addHeaders(requestBuilder);
        
        try (Response response = httpClient.newCall(requestBuilder.build()).execute()) {
            Log.d(TAG, "ğŸš€ getContentLength response code: " + response.code());
            
            if (response.isSuccessful() || response.code() == 206) {
                // ä» Content-Range è·å–æ€»å¤§å°: "bytes 0-0/1009689143"
                String contentRange = response.header("Content-Range");
                Log.d(TAG, "ğŸš€ Content-Range: " + contentRange);
                
                if (contentRange != null && contentRange.contains("/")) {
                    String[] parts = contentRange.split("/");
                    if (parts.length == 2 && !parts[1].equals("*")) {
                        long size = Long.parseLong(parts[1]);
                        Log.d(TAG, "ğŸš€ File size from Content-Range: " + size);
                        return size;
                    }
                }
                
                // å¤‡ç”¨ï¼šä» Content-Length è·å–
                String contentLengthHeader = response.header("Content-Length");
                if (contentLengthHeader != null) {
                    Log.d(TAG, "ğŸš€ Content-Length: " + contentLengthHeader);
                    return Long.parseLong(contentLengthHeader);
                }
            }
        }
        return C.LENGTH_UNSET;
    }
    
    private long openSingleConnection(DataSpec dataSpec) throws IOException {
        // å›é€€åˆ°å•è¿æ¥æ¨¡å¼
        Request.Builder requestBuilder = new Request.Builder()
            .url(uri.toString())
            .get();
        
        addHeaders(requestBuilder);
        
        if (dataSpec.position > 0) {
            requestBuilder.header("Range", "bytes=" + dataSpec.position + "-");
        }
        
        Response response = httpClient.newCall(requestBuilder.build()).execute();
        if (!response.isSuccessful()) {
            throw new IOException("HTTP error: " + response.code());
        }
        
        // å­˜å‚¨å“åº”æµ
        currentChunk = new ChunkData(0, response.body().bytes());
        currentChunkOffset = 0;
        bytesRemaining = currentChunk.data.length;
        
        return bytesRemaining;
    }
    
    private void startPrefetch() {
        for (int i = 0; i < prefetchChunks && i < numConnections; i++) {
            scheduleNextChunkDownload();
        }
    }
    
    private void scheduleNextChunkDownload() {
        if (!isOpened.get()) return;
        
        long chunkIndex = nextChunkToDownload.getAndIncrement();
        long startByte = chunkIndex * CHUNK_SIZE;
        
        if (startByte >= contentLength) {
            return; // å·²ç»åˆ°æ–‡ä»¶æœ«å°¾
        }
        
        long endByte = Math.min(startByte + CHUNK_SIZE - 1, contentLength - 1);
        
        Future<?> task = downloadExecutor.submit(() -> downloadChunk(chunkIndex, startByte, endByte));
        synchronized (downloadTasks) {
            downloadTasks.add(task);
        }
    }
    
    private void downloadChunk(long chunkIndex, long startByte, long endByte) {
        if (!isOpened.get()) return;
        
        Log.d(TAG, "ğŸš€ Downloading chunk " + chunkIndex + ": bytes=" + startByte + "-" + endByte);
        
        try {
            Request.Builder requestBuilder = new Request.Builder()
                .url(uri.toString())
                .get()
                .header("Range", "bytes=" + startByte + "-" + endByte);
            
            addHeaders(requestBuilder);
            
            Response response = httpClient.newCall(requestBuilder.build()).execute();
            
            if (response.isSuccessful() || response.code() == 206) {
                byte[] data = response.body().bytes();
                ChunkData chunk = new ChunkData(chunkIndex, data);
                
                // ç­‰å¾…æ”¾å…¥ç¼“å†²åŒºï¼ˆé˜»å¡ç›´åˆ°æœ‰ç©ºé—´ï¼‰
                while (isOpened.get()) {
                    try {
                        if (chunkBuffer.offer(chunk, 100, TimeUnit.MILLISECONDS)) {
                            Log.d(TAG, "ğŸš€ Chunk " + chunkIndex + " buffered, size=" + data.length);
                            
                            // è°ƒåº¦ä¸‹ä¸€ä¸ªå—ä¸‹è½½
                            scheduleNextChunkDownload();
                            break;
                        }
                    } catch (InterruptedException e) {
                        Thread.currentThread().interrupt();
                        break;
                    }
                }
            } else {
                Log.e(TAG, "ğŸš€ Chunk " + chunkIndex + " download failed: " + response.code());
            }
            
            response.close();
            
        } catch (Exception e) {
            if (isOpened.get()) {
                Log.e(TAG, "ğŸš€ Chunk " + chunkIndex + " download error", e);
            }
        }
    }
    
    private void addHeaders(Request.Builder builder) {
        if (defaultHeaders != null) {
            for (Map.Entry<String, String> entry : defaultHeaders.entrySet()) {
                builder.header(entry.getKey(), entry.getValue());
            }
        }
    }
    
    @Override
    public int read(@NonNull byte[] buffer, int offset, int length) throws HttpDataSource.HttpDataSourceException {
        if (bytesRemaining == 0) {
            return C.RESULT_END_OF_INPUT;
        }
        
        try {
            int bytesRead = 0;
            
            while (bytesRead < length && bytesRemaining > 0) {
                // ç¡®ä¿æœ‰å½“å‰å—å¯è¯»
                if (currentChunk == null || currentChunkOffset >= currentChunk.data.length) {
                    // éœ€è¦è·å–ä¸‹ä¸€ä¸ªå—
                    currentChunk = getNextChunk();
                    if (currentChunk == null) {
                        break; // æ²¡æœ‰æ›´å¤šæ•°æ®
                    }
                    currentChunkOffset = (int) (currentPosition % CHUNK_SIZE);
                }
                
                // ğŸ”‘ æå‰é¢„å–ï¼šå½“è¯»å–åˆ°å—çš„50%æ—¶ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„é¢„å–
                if (currentChunkOffset > currentChunk.data.length / 2) {
                    ensurePrefetch();
                }
                
                // ä»å½“å‰å—è¯»å–æ•°æ®
                int availableInChunk = currentChunk.data.length - currentChunkOffset;
                int toRead = (int) Math.min(Math.min(length - bytesRead, availableInChunk), bytesRemaining);
                
                System.arraycopy(currentChunk.data, currentChunkOffset, buffer, offset + bytesRead, toRead);
                
                currentChunkOffset += toRead;
                currentPosition += toRead;
                bytesRead += toRead;
                bytesRemaining -= toRead;
                
                bytesTransferred(toRead);
            }
            
            return bytesRead > 0 ? bytesRead : C.RESULT_END_OF_INPUT;
            
        } catch (Exception e) {
            throw new HttpDataSource.HttpDataSourceException(
                e instanceof IOException ? (IOException) e : new IOException(e), 
                new DataSpec(uri), 
                HttpDataSource.HttpDataSourceException.TYPE_READ);
        }
    }
    
    // ç¡®ä¿æœ‰è¶³å¤Ÿçš„é¢„å–ä»»åŠ¡åœ¨è¿è¡Œ
    private void ensurePrefetch() {
        int bufferedCount = chunkBuffer.size() + pendingChunks.size();
        if (bufferedCount < prefetchChunks) {
            scheduleNextChunkDownload();
        }
    }
    
    private ChunkData getNextChunk() throws InterruptedException {
        long expectedChunkIndex = nextChunkToRead.get();
        
        Log.d(TAG, "ğŸš€ getNextChunk: expecting chunk " + expectedChunkIndex);
        
        // é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ pendingChunks ä¸­
        ChunkData pending = pendingChunks.remove(expectedChunkIndex);
        if (pending != null) {
            Log.d(TAG, "ğŸš€ Found chunk " + expectedChunkIndex + " in pending");
            nextChunkToRead.incrementAndGet();
            // æ¸…ç†è¿‡æœŸçš„ pending chunks é‡Šæ”¾å†…å­˜
            cleanupOldPendingChunks(expectedChunkIndex);
            return pending;
        }
        
        // ä»ç¼“å†²åŒºè·å–å—
        int waitCount = 0;
        while (isOpened.get()) {
            ChunkData chunk = chunkBuffer.poll(200, TimeUnit.MILLISECONDS);
            if (chunk != null) {
                Log.d(TAG, "ğŸš€ Got chunk " + chunk.index + " from buffer, expecting " + expectedChunkIndex);
                
                if (chunk.index == expectedChunkIndex) {
                    nextChunkToRead.incrementAndGet();
                    cleanupOldPendingChunks(expectedChunkIndex);
                    return chunk;
                } else if (chunk.index > expectedChunkIndex) {
                    // å—é¡ºåºä¸å¯¹ï¼Œå­˜å…¥ pendingChunks
                    pendingChunks.put(chunk.index, chunk);
                    Log.d(TAG, "ğŸš€ Chunk " + chunk.index + " stored in pending, waiting for " + expectedChunkIndex);
                }
                // å¦‚æœ chunk.index < expectedChunkIndexï¼Œä¸¢å¼ƒï¼ˆå·²ç»è¿‡æ—¶ï¼‰
            } else {
                waitCount++;
                if (waitCount % 10 == 0) {
                    Log.d(TAG, "ğŸš€ Waiting for chunk " + expectedChunkIndex + ", waited " + (waitCount * 200) + "ms");
                }
            }
            
            // æ£€æŸ¥æ˜¯å¦å·²ç»ä¸‹è½½å®Œæˆ
            if (currentPosition >= contentLength) {
                Log.d(TAG, "ğŸš€ Reached end of content");
                return null;
            }
            
            // è¶…æ—¶ä¿æŠ¤ï¼šç­‰å¾…è¶…è¿‡10ç§’å°±æ”¾å¼ƒ
            if (waitCount > 50) {
                Log.e(TAG, "ğŸš€ Timeout waiting for chunk " + expectedChunkIndex);
                return null;
            }
        }
        return null;
    }
    
    // æ¸…ç†è¿‡æœŸçš„ pending chunks é‡Šæ”¾å†…å­˜
    private void cleanupOldPendingChunks(long currentIndex) {
        // ä½¿ç”¨ä¼ ç»Ÿå¾ªç¯é¿å… lambda å…¼å®¹æ€§é—®é¢˜
        java.util.Iterator<java.util.Map.Entry<Long, ChunkData>> iterator = pendingChunks.entrySet().iterator();
        while (iterator.hasNext()) {
            java.util.Map.Entry<Long, ChunkData> entry = iterator.next();
            if (entry.getKey() < currentIndex) {
                iterator.remove();
            }
        }
    }
    
    @Nullable
    @Override
    public Uri getUri() {
        return uri;
    }
    
    @Override
    public void close() {
        Log.d(TAG, "ğŸš€ Closing parallel data source");
        
        isOpened.set(false);
        
        // å–æ¶ˆæ‰€æœ‰ä¸‹è½½ä»»åŠ¡
        synchronized (downloadTasks) {
            for (Future<?> task : downloadTasks) {
                task.cancel(true);
            }
            downloadTasks.clear();
        }
        
        // å…³é—­çº¿ç¨‹æ± 
        if (downloadExecutor != null) {
            downloadExecutor.shutdownNow();
            downloadExecutor = null;
        }
        
        // æ¸…ç©ºç¼“å†²åŒº
        if (chunkBuffer != null) {
            chunkBuffer.clear();
        }
        pendingChunks.clear();
        currentChunk = null;
        
        // ğŸ”‘ å®‰å…¨è°ƒç”¨ transferEnded - åªæœ‰åœ¨ uri ä¸ä¸ºç©ºæ—¶æ‰è°ƒç”¨
        if (uri != null) {
            try {
                transferEnded();
            } catch (Exception e) {
                Log.w(TAG, "transferEnded error (ignored)", e);
            }
        }
    }
    
    /**
     * æ•°æ®å—
     */
    private static class ChunkData {
        final long index;
        final byte[] data;
        
        ChunkData(long index, byte[] data) {
            this.index = index;
            this.data = data;
        }
    }
    
    /**
     * å·¥å‚ç±»
     */
    public static class Factory implements androidx.media3.datasource.DataSource.Factory {
        
        private final OkHttpClient httpClient;
        private final Map<String, String> defaultHeaders;
        
        public Factory(OkHttpClient httpClient, Map<String, String> defaultHeaders) {
            this.httpClient = httpClient;
            this.defaultHeaders = defaultHeaders;
        }
        
        @NonNull
        @Override
        public ParallelDataSource createDataSource() {
            return new ParallelDataSource(httpClient, defaultHeaders);
        }
    }
}
